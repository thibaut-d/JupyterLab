{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Add topics to scholarly articles on Wikidata.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP7lcPRwCJkh"
      },
      "source": [
        "# Add topics to scholarly articles on Wikidata\n",
        "\n",
        "The goal of this bot is to add main subject (P921) to scholarly articles.\n",
        "\n",
        "The bot search in scholarly articles' labels for a list of keywords. Each keyword is associated with an item that can be assumed to be one of the main subjects when found in the title of the publication.\n",
        "\n",
        "For example :\n",
        "\n",
        "- An article with \"Stainless Steel\" in the title or description is assumed to have a main subject (P921) → stainless steel (Q172587)\n",
        "- An article with \"Ebola virus\" in the title or description is assumed to have a main subject (P921) → Ebola virus (Q10538943)\n",
        "\n",
        "The metadata of scholarly articles in Wikipedias are virtually impossible to maintain by hand because the rate of creation of these articles exceed the capacity and willingness of the community. So that adding such automation is the only way we can maintain these data.\n",
        "\n",
        "The code is released under CC BY SA. Author is Thibdx.\n",
        "Feel free to fork and adapt the code if you know what you do and already have a bot account.\n",
        "\n",
        "More on : https://www.wikidata.org/wiki/Wikidata:AddScholarTopics_Bot\n",
        "\n",
        "*NB : sorry for the heavy 'string {}'.format(var) instead of f'string {var}', the bot has been adapted to run on older Python on Toolforge.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH8s-4vPCJkl"
      },
      "source": [
        "import pywikibot\n",
        "from pywikibot import pagegenerators as pg\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime\n",
        "\n",
        "\n",
        "wikidata_site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
        "wikidata_repo = wikidata_site.data_repository()\n",
        "\n",
        "# Global variables. Shall not be changed.\n",
        "Pid = 'P921'  # This is the property ID of \"main topic\"\n",
        "bot_page = 'Wikidata:AddScholarTopics_Script' # Where the user contributed dict is stored\n",
        "topics_dict_id = 'topics_dict' #The id of the pre bloc that contains the topics dict\n",
        "exclusions_dict_id = 'exclusions_dict' #The id of the pre bloc that contains the exclusions dict\n",
        "regex_dict_id = 'regex_dict' #The id of the pre bloc that contains the regex dict\n",
        "type_filter = 'haswbstatement:P31=Q13442814' # Filters the articles that are scolarly articles. It may be enlarged to other scientific publications in the future."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ons-Oyn3CJkm"
      },
      "source": [
        "# We want to retrieve topics_dict and exclusion_dict from : https://www.wikidata.org/wiki/Wikidata:AddScholarTopics_Bot \n",
        "# We use BeautifullSoup to search for the text : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n",
        "def get_dict(bot_page,dict_id):\n",
        "    \n",
        "    '''\n",
        "    This function gather the dicts from a Wikidata page @bot_page.\n",
        "    \n",
        "    Here is the structure of the topic dict :\n",
        "    It reads : Items having low dose naltrexone in the title are assumed to have Q5259325 as a main topic.\n",
        "\n",
        "    topics_dict = {\n",
        "        'low dose naltrexone':'Q5259325',\n",
        "        'Behçet disease':'Q911427',\n",
        "        'Ehlers Danlos':'Q1141499',\n",
        "        'fibromyalgia':'Q540571',\n",
        "    }\n",
        "\n",
        "    Here is the structure of an exclusion list :\n",
        "    It reads : Scolarly article Q46788624 does not have Q797668 or Q5384031 as main topics.\n",
        "    This may be usefull in very spêcific cases.\n",
        "    \n",
        "    exclusions_dict = {\n",
        "        'Q46788624':['Q797668','Q5384031'] \n",
        "    }\n",
        "    '''\n",
        "    \n",
        "    # The lists are user contributed in this Wikidata page.\n",
        "    # Alaways verifiy the consitency of new data added to the lists before launching the bot.\n",
        "    page = pywikibot.Page(wikidata_site, bot_page) \n",
        "    \n",
        "    print('\\nTrying to get the dict at id = \\\"{dict_id}\\\" in {bot_page}'.format(dict_id = dict_id, bot_page = bot_page))\n",
        "    \n",
        "    if page.exists():\n",
        "        page_txt = page.text\n",
        "        soup = BeautifulSoup(page_txt,'lxml')\n",
        "        \n",
        "        # The dicts are located inside <pre id=...> </pre> tags.\n",
        "        try:\n",
        "            topics_dict_txt = soup.find('pre' , attrs = { 'id' : dict_id }).text        \n",
        "        # If soup.find or eval fail, the bot is stopped. \n",
        "        # If you recieve this message please double check the page's wikicode.\n",
        "        except:\n",
        "            print('\\n!!!!!!!!!!!! \\nFound {} but not the dict\\n'.format(bot_page))\n",
        "            return False\n",
        "        \n",
        "        # check if the dict can be evaluated\n",
        "        try:\n",
        "            print('\\nFound bot\\'s page with content :\\n{}'.format(topics_dict_txt))\n",
        "            return eval(topics_dict_txt) \n",
        "        except:\n",
        "            print('\\n!!!!!!!!!!!! \\nFound {} & dict, but syntax is invalid...\\n'.format(bot_page))\n",
        "            return False\n",
        "    \n",
        "    # If the page is not found, it may have been moved or the variable may be settled to a wrong page.\n",
        "    else :\n",
        "        print('\\n!!!!!!!!!!!! \\nCould not find {}\\n'.format(bot_page))\n",
        "        return False\n",
        "\n",
        "#Test\n",
        "#print(get_dict(bot_page,topics_dict_id))\n",
        "#print(get_dict(bot_page,exclusions_dict_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2FwwjN2GCJkn"
      },
      "source": [
        "# The following query gather scholarly articles with a keyword in the label.\n",
        "# Only articles that don't yet have the associated main topic are gathered.\n",
        "\n",
        "def get_articles(keyword,target_Qid,Pid,type_filter):\n",
        "    \n",
        "    \"\"\"\n",
        "    This function get the articles that have :\n",
        "        - @keyword in the title \n",
        "        - and where @property_id is not yet settled to @Qid\n",
        "        \n",
        "    @keyword is a string (ie. fibromyalgia)\n",
        "    @target_Qid is a string (ie. Q12345)\n",
        "    @Pid is a string (ie. P921)\n",
        "    \"\"\"\n",
        "  \n",
        "    query = '''\n",
        "    SELECT DISTINCT ?item ?itemLabel \n",
        "      WHERE {{\n",
        "        hint:Query hint:optimizer \"None\".\n",
        "        SERVICE wikibase:mwapi {{\n",
        "        bd:serviceParam wikibase:api \"Search\";\n",
        "                      wikibase:endpoint \"www.wikidata.org\";\n",
        "                      mwapi:srsearch \"{keyword} {type_filter}\".\n",
        "        ?title wikibase:apiOutput mwapi:title.\n",
        "      }}\n",
        "    BIND(IRI(CONCAT(STR(wd:), ?title)) AS ?item)\n",
        "    FILTER NOT EXISTS {{ ?item wdt:{Pid} wd:{target_Qid}. }}\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }} \n",
        "    }}\n",
        "    '''.format(\n",
        "        keyword = keyword, \n",
        "        type_filter = type_filter,\n",
        "        Pid = Pid, \n",
        "        target_Qid = target_Qid\n",
        "    )\n",
        "    \n",
        "    print('Getting articles with \\\"{keyword}\\\" in the label but {Pid} not yet settled to {target_Qid}...'.format(\n",
        "        keyword = keyword, \n",
        "        Pid = Pid, \n",
        "        target_Qid = target_Qid\n",
        "    ))\n",
        "    \n",
        "    generator = pg.WikidataSPARQLPageGenerator(query, site=wikidata_site)\n",
        "    \n",
        "    print('Generator generated...')\n",
        "    \n",
        "    return generator\n",
        "    \n",
        "#Testing        \n",
        "#test_generator = get_articles('Ehlers Danlos','Q1141499',Pid,type_filter)\n",
        "#for item in test_generator :\n",
        "#    print(str(item) + ' - ' + str(item.get()['labels']['en']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5c3bOr7CJko"
      },
      "source": [
        "def is_not_excluded(item,target_Qid,exclusions_dict):\n",
        "    '''\n",
        "    This function checks if the item/target pair is in the exlusion list.\n",
        "    If return True if the item/target pair is not in the list.\n",
        "    Else, it return false.\n",
        "    '''\n",
        "    item_Qid = item.id\n",
        "    \n",
        "    if item_Qid in exclusions_dict:\n",
        "        for Qid in exclusions_dict[item_Qid]:\n",
        "            if Qid == target_Qid:\n",
        "                print('{target_Qid} is in exclusions dict and will not be added to {item_Qid}.'.format(target_Qid = target_Qid, item_Qid = item_Qid))\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    else:\n",
        "        return True\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOAiqHZ6CJko"
      },
      "source": [
        "def get_item_label(item,lang = 'en'):\n",
        "    item.get()  # you need to call it to access any data.\n",
        "    if lang in item.labels:\n",
        "        return item.labels[lang]\n",
        "    else:\n",
        "        return 'No label in {}'.format(lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CosIO2NpCJko"
      },
      "source": [
        "# import re # for testing ony this cell\n",
        "\n",
        "def error_shield(item_label,keyword,regex_dict):\n",
        "    keyword = keyword.lower()\n",
        "    if keyword in regex_dict :\n",
        "        for regex in regex_dict[keyword] :\n",
        "            matching = re.search(regex.lower(), item_label.lower())\n",
        "            if matching == None :\n",
        "                return False\n",
        "            else :\n",
        "                return True\n",
        "    else :\n",
        "        matching = re.search(keyword, item_label.lower())\n",
        "        if matching == None :\n",
        "            return False\n",
        "        else :\n",
        "            return True\n",
        "\n",
        "# Test\n",
        "# item_label = 'Il y a des vitamines dans la bananne'\n",
        "# keyword = 'Vitamin B'\n",
        "# regex_dict = { 'vitamin b':['vitamine?s?[ \\-_]?b[1-9]*','b[1-9]*[ \\-_]?vitamine?s?'] }\n",
        "# error_shield(item_label,keyword,regex_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfvlGIsdCJkp"
      },
      "source": [
        "\n",
        "def set_claim_item(item,Pid,target_Qid,log_file):\n",
        "    \n",
        "    \"\"\"\n",
        "    This is a generic function to settle a claim when the property type is item.\n",
        "    @item is an pywikibot.ItemPage object\n",
        "    @Pid is a string (ie: P921)\n",
        "    @target_Qid is a string (ie: Q12345)\n",
        "    \"\"\"\n",
        "    \n",
        "    claim = pywikibot.Claim(wikidata_repo, Pid)\n",
        "    \n",
        "    target_item = pywikibot.ItemPage(wikidata_repo, target_Qid, 0)\n",
        "    claim.setTarget(target_item)\n",
        "    \n",
        "    item_label = get_item_label(item)\n",
        "    target_item_label = get_item_label(target_item)\n",
        "    \n",
        "    log_dict = {\n",
        "        'item_id' : str(item.id),\n",
        "        'item_label' : str(get_item_label(item)),\n",
        "        'property_id' : str(Pid),\n",
        "        'target_item_id' : str(target_item.id),\n",
        "        'target_item_label' : str(get_item_label(target_item)),\n",
        "        'time' : str(datetime.datetime.now()),\n",
        "    }\n",
        "    \n",
        "    #message = 'Item ' + str(item.id) + ' (' + str(item_label) + ')' + ' > setting ' + str(Pid) + ' to ' + str(target_item.id) + ' (' + str(target_item_label) + ')'\n",
        "    message = 'Item {item_id} ( {item_label} ) > setting {property_id} to {target_item_id} ( {target_item_label} )'.format(\n",
        "        item_id = log_dict['item_id'],\n",
        "        item_label = log_dict['item_label'],\n",
        "        property_id = log_dict['property_id'],\n",
        "        target_item_id = log_dict['target_item_id'],\n",
        "        target_item_label = log_dict['target_item_label'],\n",
        "    )\n",
        "    \n",
        "    item.addClaim(claim, summary=message)\n",
        "    \n",
        "    print(message)\n",
        "    \n",
        "    log_file.write('\\n{}'.format(str(log_dict)))\n",
        "    \n",
        "    return claim\n",
        "\n",
        "#Testing this on Wikidata sandbox\n",
        "#wikidata_sandbox_item = pywikibot.ItemPage(wikidata_repo, 'Q4115189', 0)\n",
        "#sandbox_id = 'Q842193'\n",
        "#set_claim_item(wikidata_sandbox_item,Pid,sandbox_id,log_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qK89qXe0CJkp"
      },
      "source": [
        "def set_claims_for_generator(generator,keyword,Pid,target_Qid,exclusions_dict,regex_dict,log_file):\n",
        "    \n",
        "    \"\"\"\n",
        "    Set a claim with @Pid settled to @target_Qid for all items in the @generator.\n",
        "    \n",
        "    @generator : pg.WikidataSPARQLPageGenerator object\n",
        "    @Pid : string (ex: P921)\n",
        "    @target_Qid  string (ex: Q12345)\n",
        "    \"\"\"\n",
        "\n",
        "    i=0\n",
        "    for item in generator:\n",
        "        i+=1\n",
        "        #print(item) #uncomment this for debug only \n",
        "        if is_not_excluded(item,target_Qid,exclusions_dict) :\n",
        "            item_label = get_item_label(item)\n",
        "            if error_shield(item_label,keyword,regex_dict) :\n",
        "                set_claim_item(item,Pid,target_Qid,log_file)\n",
        "            else :\n",
        "                print('The keyword {} or its defined regexs are not in the label. Skipping...'.format(keyword))\n",
        "        else :\n",
        "            print('{} is in exclusions dict and will not be added.'.format(target_Qid))\n",
        "    \n",
        "    print('{i} claims were setteled with {Pid} to {target_Qid}'.format(i = i, Pid = Pid, target_Qid = target_Qid))\n",
        "    return i   \n",
        "    #beware, after a for loop, a generator is not usable anymore.\n",
        "    \n",
        "# Testing (please interrupt the operation before the end if many items are to be settled)\n",
        "# keyword = 'Ehlers Danlos'\n",
        "# target_Qid = 'Q1141499'\n",
        "# generator = get_articles(keyword,target_Qid,Pid)\n",
        "# set_claims_for_generator(generator,Pid,target_Qid,exclusion_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFoBWoi3CJkp"
      },
      "source": [
        "def add_scholar_topics(Pid,bot_page,type_filter):\n",
        "    \n",
        "    \"\"\"\n",
        "    This is the main function of this script.\n",
        "    It iterates over the dict + over SPARQL results and add the main topics for each article\n",
        "    \n",
        "    @topict_dict is a dicitonary : {keyword:target_Qid,}\n",
        "    \n",
        "    \"\"\"\n",
        "    topics_dict = get_dict(bot_page,topics_dict_id)\n",
        "    exclusions_dict = get_dict(bot_page,exclusions_dict_id)\n",
        "    regex_dict = get_dict(bot_page,regex_dict_id)\n",
        "    log_file = open(\"addScholarTopics.log\", \"a\")\n",
        "    \n",
        "    i=0\n",
        "    for keyword in topics_dict:\n",
        "        i+=1\n",
        "        target_Qid = topics_dict[keyword]\n",
        "        generator = get_articles(keyword,target_Qid,Pid,type_filter)\n",
        "        \n",
        "        set_claims_for_generator(generator,keyword,Pid,target_Qid,exclusions_dict,regex_dict,log_file)\n",
        "    \n",
        "    print('\\nDone ! {} keyword/topic pairs were checked and settled...'.format(i))\n",
        "    \n",
        "    log_file.close()\n",
        "    \n",
        "    return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELkz4kGACJkq"
      },
      "source": [
        "#OK... Let's run the script !\n",
        "\n",
        "add_scholar_topics(Pid,bot_page,type_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w570rwVICJkq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O73liEpkCJkq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q40L4YCkCJkq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}